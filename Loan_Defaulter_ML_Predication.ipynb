{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmhjP3xp4N3YRAEAT7c3Tc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder"
      ],
      "metadata": {
        "id": "efcvUJ6iUtlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUK4n7FtJVff"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/Loan_default.csv\") # load data set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df  = df.drop([\"LoanID\"],axis=1) # there is no use of LoanId\n"
      ],
      "metadata": {
        "id": "9BlwxV-3JpAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "PbsKSitHJ0lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "u8V-CpGCJ-Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ordinal Encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "IyrV8oBvKCxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categorical columns to analyze\n",
        "categorical_columns = [\n",
        "    'Education', 'EmploymentType', 'MaritalStatus',\n",
        "    'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner'\n",
        "]\n",
        "\n",
        "# Iterate through each column and calculate the counts, percentages, and averages\n",
        "for col in categorical_columns:\n",
        "    print(f\"\\nAnalysis for {col}:\\n\")\n",
        "\n",
        "    # Group by the column and 'Default', then count occurrences\n",
        "    counts = df.groupby([col, 'Default']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Calculate total counts for each category\n",
        "    total_counts = counts.sum(axis=1)\n",
        "\n",
        "    # Calculate percentages of defaults and non-defaults\n",
        "    percentages = (counts.div(total_counts, axis=0) * 100).round(2)\n",
        "\n",
        "    # Add an average column for 'Default' weighted by the counts\n",
        "    averages = df.groupby(col)['Default'].mean().round(2)\n",
        "\n",
        "    # Combine counts, percentages, and averages into one table\n",
        "    summary = counts.copy()\n",
        "    summary['Total'] = total_counts\n",
        "    summary['Default %'] = percentages[1] if 1 in percentages.columns else 0\n",
        "    summary['Non-Default %'] = percentages[0] if 0 in percentages.columns else 0\n",
        "    # summary['Default Rate (Average)'] = averages\n",
        "\n",
        "    # Print the summary\n",
        "    print(summary)\n"
      ],
      "metadata": {
        "id": "dsdARF3EKMLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Ordinal Encoding instead of One hot encoding\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Based on statistical analysis, the categories in certain variables exhibit a clear order in their relationship with the target variable (Default). For example, in the \"Education\" variable, the default rate decreases as the education level increases (High School: 12.88%, Bachelor's: 12.10%, Master's: 10.87%, PhD: 10.59%). This trend is statistically significant, demonstrating an inherent order among the categories. By applying Ordinal Encoding, we can capture this order and utilize the progression between categories effectively, which enhances the model's ability to predict loan defaults."
      ],
      "metadata": {
        "id": "ESArFNABMFlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hfyQ9w0EKTyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# List of categorical columns to analyze\n",
        "categorical_columns = [\n",
        "    'Education', 'EmploymentType', 'MaritalStatus',\n",
        "    'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner'\n",
        "]\n",
        "\n",
        "# Initialize an OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "\n",
        "# Dynamically generate ordinal mappings\n",
        "ordinal_mappings = {}\n",
        "\n",
        "for col in categorical_columns:\n",
        "    # Group by the column and calculate the mean default rate\n",
        "    default_rates = df.groupby(col)['Default'].mean().sort_values()\n",
        "\n",
        "    # Generate the ordinal mapping dynamically based on default rates\n",
        "    mapping = {category: rank for rank, category in enumerate(default_rates.index, start=1)}\n",
        "    ordinal_mappings[col] = mapping\n",
        "\n",
        "    # Apply the mapping to the column in the DataFrame\n",
        "    df[col] = df[col].map(mapping)\n",
        "\n",
        "    # print(f\"Ordinal mapping for {col}: {mapping}\")\n",
        "\n",
        "# Display the first few rows of the updated DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "mlEBXOhHNGA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q2uutPDTla_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Test Split"
      ],
      "metadata": {
        "id": "A6QVhj6dOt5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'Default' is your target variable\n",
        "X = df.drop('Default', axis=1)\n",
        "y = df['Default']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "7QbApkEMOYDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Scaling"
      ],
      "metadata": {
        "id": "iPZFCR2yO5x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Now X_train_scaled and X_test_scaled contain the normalized data\n",
        "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
        "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n"
      ],
      "metadata": {
        "id": "oi9d6fbtrLkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_outliers_iqr(data):\n",
        "    Q1 = np.percentile(data, 25)\n",
        "    Q3 = np.percentile(data, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "    return outliers\n",
        "\n",
        "# Example usage for a specific column (replace 'LoanAmount' with the column you want to analyze)\n",
        "numerical_columns = ['LoanAmount', 'LoanTerm', 'CreditScore']\n",
        "#Check if all numerical_columns are in df\n",
        "for col in numerical_columns:\n",
        "    if col not in df.columns:\n",
        "        print(f\"Warning: Column '{col}' not found in DataFrame. Skipping...\")\n",
        "        continue  # Skip to the next column\n",
        "    outliers = find_outliers_iqr(df[col])\n",
        "    print(f\"Outliers in {col}: {outliers}\")\n",
        "    print(f\"Number of outliers in {col}: {len(outliers)}\")\n"
      ],
      "metadata": {
        "id": "JhSTX9K-S2UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply logistic regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logreg = LogisticRegression(random_state=42)  # You can adjust hyperparameters as needed\n",
        "\n",
        "# Train the model using the scaled training data\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "y_pred = logreg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model (example: accuracy)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Logistic Regression: {accuracy}\")\n",
        "\n",
        "# You can further evaluate using other metrics like precision, recall, F1-score, etc.\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "9H6tvkA9rbtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "\n",
        "\n",
        "# Get feature importances (coefficients for logistic regression)\n",
        "feature_importances = logreg.coef_[0]\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importances in a user-friendly format\n",
        "for index, row in feature_importance_df.iterrows():\n",
        "    print(f\"{row['Feature']} feature importance: {row['Importance']:.4f}\")\n"
      ],
      "metadata": {
        "id": "SKzOrZyQrjNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Decision Tree Classifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)  # You can adjust hyperparameters\n",
        "\n",
        "# Train the model\n",
        "dt_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = dt_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Accuracy of Decision Tree Classifier: {accuracy_dt}\")\n",
        "print(classification_report(y_test, y_pred_dt))\n"
      ],
      "metadata": {
        "id": "TbC1iaHvr0YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply naive baise\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize the Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_gnb = gnb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
        "print(f\"Accuracy of Gaussian Naive Bayes: {accuracy_gnb}\")\n",
        "print(classification_report(y_test, y_pred_gnb))\n"
      ],
      "metadata": {
        "id": "MgDJ6OilZmWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)  # You can adjust hyperparameters\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Accuracy of Random Forest Classifier: {accuracy_rf}\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "5CjqYPCzsA6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance for Random Forest\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming rf_classifier is your trained RandomForestClassifier model\n",
        "# and X_train is your training features DataFrame\n",
        "\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importances in a user-friendly format\n",
        "for index, row in feature_importance_df.iterrows():\n",
        "    print(f\"{row['Feature']} feature importance: {row['Importance']:.4f}\")\n"
      ],
      "metadata": {
        "id": "nuLW4611sF88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}